{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6a5e3c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 0. Imports and settings\n",
    "# =========================================================\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98103a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-0cb59023-d423-4541-be25-4db58ca12b55\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-0cb59023-d423-4541-be25-4db58ca12b55\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e4a482b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c5a5ccb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/USER/Documents/Machine_Learning/Deep_Learning/Waqar_hussain/normalized_features.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1516361782.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"C:/Users/USER/Documents/Machine_Learning/Deep_Learning/Waqar_hussain/normalized_features.xlsx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1551\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m     ) as handle:\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/USER/Documents/Machine_Learning/Deep_Learning/Waqar_hussain/normalized_features.xlsx'"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(\"C:/Users/USER/Documents/Machine_Learning/Deep_Learning/Waqar_hussain/normalized_features.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e072f585",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b2f0919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training / validation / test fractions\n",
    "TEST_FRAC = 0.15          # fraction of full dataset for test\n",
    "VALID_FRAC = 0.15         # fraction of full dataset for validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "484e3d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PCA to reduce dimensionality if you have many features and training is slow.\n",
    "USE_PCA = True\n",
    "PCA_COMPONENTS = 15      # set to None or <= number of features; tweak as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a306f2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model & training hyperparameters (you can tune these)\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-3\n",
    "DROPOUT_RATE = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "155d48b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output artifact paths\n",
    "MODEL_DIR = \"./model_artifacts\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, \"dnn_model.h5\")\n",
    "BEST_MODEL_PATH = os.path.join(MODEL_DIR, \"dnn_model_best.h5\")\n",
    "ARTIFACTS_PATH = os.path.join(MODEL_DIR, \"preprocessing_artifacts.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b81631e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow/Keras detected. Will train using Keras DNN.\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 1. Environment check: try to import TensorFlow/Keras\n",
    "# =========================================================\n",
    "USE_KERAS = False\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers, callbacks, models\n",
    "    print(\"TensorFlow/Keras detected. Will train using Keras DNN.\")\n",
    "    USE_KERAS = True\n",
    "except Exception as e:\n",
    "    print(\"TensorFlow not available or failed to import. Will fallback to scikit-learn MLP.\")\n",
    "    print(\"Import error:\", e)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# sklearn fallback\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "849ef69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your CSV file (change if needed)\n",
    "CSV_PATH = 'normalized_features_16.csv'\n",
    "#r'C:\\Users\\USER\\Documents\\Machine Learning\\Deep Learning\\Waqar_hussain\\data\\normalized_features_16.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b5fcc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset from: normalized_features_16.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "CSV file not found at: normalized_features_16.csv\nCurrent working directory: /content\nPlease verify CSV_PATH or provide the correct path to the CSV file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2026138054.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Resolved CSV_PATH to absolute path: {CSV_PATH}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \t\traise FileNotFoundError(\n\u001b[0m\u001b[1;32m     14\u001b[0m                         \u001b[0;34mf\"CSV file not found at: {CSV_PATH}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                         \u001b[0;34mf\"Current working directory: {os.getcwd()}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: CSV file not found at: normalized_features_16.csv\nCurrent working directory: /content\nPlease verify CSV_PATH or provide the correct path to the CSV file."
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 2. Load the dataset\n",
    "# =========================================================\n",
    "print(f\"\\nLoading dataset from: {CSV_PATH}\")\n",
    "\n",
    "# Check that the CSV exists and try to resolve relative paths to give a clear error if not found\n",
    "if not os.path.exists(CSV_PATH):\n",
    "\talt_path = os.path.join(os.getcwd(), CSV_PATH)\n",
    "\tif os.path.exists(alt_path):\n",
    "\t\tCSV_PATH = alt_path\n",
    "\t\tprint(f\"Resolved CSV_PATH to absolute path: {CSV_PATH}\")\n",
    "\telse:\n",
    "\t\traise FileNotFoundError(\n",
    "\t\t\tf\"CSV file not found at: {CSV_PATH}\\n\"\n",
    "\t\t\tf\"Current working directory: {os.getcwd()}\\n\"\n",
    "\t\t\t\"Please verify CSV_PATH or provide the correct path to the CSV file.\"\n",
    "\t\t)\n",
    "\n",
    "# Read CSV (explicitly surface errors if they occur)\n",
    "try:\n",
    "\tdf = pd.read_csv(CSV_PATH)\n",
    "\tprint(\"Loaded. Shape:\", df.shape)\n",
    "except Exception as e:\n",
    "\traise RuntimeError(f\"Failed to read CSV at {CSV_PATH}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e36f3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(f\"\\nLoading dataset from: {CSV_PATH}\")\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"Loaded. Shape:\", df.shape)\n",
    "print(\"Columns sample (first 30):\", df.columns.tolist()[:30])\n",
    "\n",
    "# =========================================================\n",
    "# 3. Quick EDA: shape, dtypes, missing values, target guess\n",
    "# =========================================================\n",
    "print(\"\\n=== Quick EDA ===\")\n",
    "print(\"Head (5 rows):\")\n",
    "display_cols = df.columns.tolist()[:40]\n",
    "print(df.head()[display_cols])\n",
    "\n",
    "print(\"\\nInfo:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nNumeric summary (first 10 rows of .describe()):\")\n",
    "print(df.describe().transpose().head(10))\n",
    "\n",
    "# Missing values\n",
    "missing = df.isnull().sum()\n",
    "missing_nonzero = missing[missing > 0]\n",
    "if len(missing_nonzero) > 0:\n",
    "    print(\"\\nColumns with missing values:\")\n",
    "    print(missing_nonzero)\n",
    "else:\n",
    "    print(\"\\nNo missing values detected.\")\n",
    "\n",
    "# Identify target column: prefer common names, otherwise last column\n",
    "candidate_names = [\"target\", \"label\", \"class\", \"y\", \"outcome\", \"Label\"]\n",
    "target_col = None\n",
    "for name in candidate_names:\n",
    "    if name in df.columns:\n",
    "        target_col = name\n",
    "        break\n",
    "if target_col is None:\n",
    "    target_col = df.columns[-1]\n",
    "    print(f\"No common target names found. Using last column as target: '{target_col}'\")\n",
    "else:\n",
    "    print(f\"Using detected target column: '{target_col}'\")\n",
    "\n",
    "# Show distribution of target\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(df[target_col].value_counts().sort_index())\n",
    "print(df[target_col].value_counts(normalize=True).sort_index())\n",
    "\n",
    "# Visual: target distribution (if not too many classes)\n",
    "num_classes = df[target_col].nunique()\n",
    "if num_classes <= 50:\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.countplot(x=target_col, data=df, order=df[target_col].value_counts().index)\n",
    "    plt.title(\"Target class distribution\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Large number of classes ({num_classes}) — skipping full count plot.\")\n",
    "\n",
    "# =========================================================\n",
    "# 4. Preprocessing: separate X,y, encode, scale, optional PCA\n",
    "# =========================================================\n",
    "print(\"\\n=== Preprocessing ===\")\n",
    "X = df.drop(columns=[target_col]).copy()\n",
    "y = df[target_col].copy()\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)\n",
    "\n",
    "# Handle non-numeric features in X\n",
    "non_numeric_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "if non_numeric_cols:\n",
    "    print(\"Non-numeric columns detected:\", non_numeric_cols)\n",
    "    # Simple approach: label-encode each column (alternatively: one-hot if few categories)\n",
    "    for col in non_numeric_cols:\n",
    "        le_tmp = LabelEncoder()\n",
    "        X[col] = le_tmp.fit_transform(X[col].astype(str))\n",
    "    print(\"Encoded non-numeric feature columns with LabelEncoder.\")\n",
    "else:\n",
    "    print(\"All features appear numeric.\")\n",
    "\n",
    "# Scale features (StandardScaler). Even if features were normalized, scaling is safe.\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"Applied StandardScaler to features.\")\n",
    "\n",
    "# Optional: PCA for dimensionality reduction (speeds training, may lose some info)\n",
    "use_pca = USE_PCA and (PCA_COMPONENTS is not None)\n",
    "pca = None\n",
    "if use_pca:\n",
    "    from sklearn.decomposition import PCA\n",
    "    n_features = X_scaled.shape[1]\n",
    "    pca_n = PCA_COMPONENTS if PCA_COMPONENTS < n_features else min(n_features, PCA_COMPONENTS)\n",
    "    print(f\"Applying PCA: reducing from {n_features} -> {pca_n} components\")\n",
    "    pca = PCA(n_components=pca_n, random_state=RANDOM_STATE)\n",
    "    X_reduced = pca.fit_transform(X_scaled)\n",
    "    input_dim = X_reduced.shape[1]\n",
    "else:\n",
    "    X_reduced = X_scaled\n",
    "    input_dim = X_reduced.shape[1]\n",
    "\n",
    "print(\"Final input dimension for model:\", input_dim)\n",
    "\n",
    "# Encode the target y\n",
    "# If target is categorical string or integer categories, LabelEncoder -> [0..C-1]\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y.astype(str))\n",
    "class_names = label_encoder.classes_\n",
    "n_classes = len(class_names)\n",
    "print(f\"Detected {n_classes} unique classes in target.\")\n",
    "\n",
    "# If user intends binary classification but labels are 2 categories, ensure correct shapes\n",
    "binary_classification = (n_classes == 2)\n",
    "\n",
    "# =========================================================\n",
    "# 5. Train / Validation / Test split (stratified)\n",
    "# =========================================================\n",
    "print(\"\\n=== Train / Val / Test split ===\")\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_reduced, y_encoded, test_size=TEST_FRAC, random_state=RANDOM_STATE,\n",
    "    stratify=y_encoded if n_classes > 1 else None\n",
    ")\n",
    "\n",
    "# compute fraction of temp to allocate to validation so that VALID_FRAC is of original data\n",
    "val_fraction_of_temp = VALID_FRAC / (1.0 - TEST_FRAC)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=val_fraction_of_temp, random_state=RANDOM_STATE,\n",
    "    stratify=y_temp if n_classes > 1 else None\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Val shape:\", X_val.shape, y_val.shape)\n",
    "print(\"Test shape:\", X_test.shape, y_test.shape)\n",
    "\n",
    "# =========================================================\n",
    "# 6. Handle class imbalance (optional)\n",
    "# =========================================================\n",
    "print(\"\\n=== Class imbalance handling ===\")\n",
    "class_counts = np.bincount(y_encoded)\n",
    "print(\"Class counts:\", class_counts)\n",
    "if class_counts.min() / class_counts.max() < 0.5:\n",
    "    print(\"Noticeable class imbalance detected. We'll compute class weights for training.\")\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weight_dict = {i: w for i, w in enumerate(class_weights)}\n",
    "    print(\"Computed class_weight dict (example):\", dict(list(class_weight_dict.items())[:10]))\n",
    "else:\n",
    "    class_weight_dict = None\n",
    "    print(\"Class distribution relatively balanced or acceptable.\")\n",
    "\n",
    "# =========================================================\n",
    "# 7. Build model (Keras DNN if available; else sklearn MLP)\n",
    "# =========================================================\n",
    "print(\"\\n=== Model building & training ===\")\n",
    "if USE_KERAS:\n",
    "    # Build Keras sequential model — configurable architecture\n",
    "    def build_dnn(input_dim, n_classes, dropout_rate=DROPOUT_RATE, lr=LEARNING_RATE):\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Input(shape=(input_dim,)))\n",
    "        # Hidden layers — adjust sizes as needed\n",
    "        model.add(layers.Dense(256, activation='relu'))\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "        model.add(layers.Dense(128, activation='relu'))\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "        model.add(layers.Dense(64, activation='relu'))\n",
    "        # Output\n",
    "        if n_classes == 2:\n",
    "            model.add(layers.Dense(1, activation='sigmoid'))\n",
    "            loss = 'binary_crossentropy'\n",
    "            metrics = ['accuracy']\n",
    "        else:\n",
    "            model.add(layers.Dense(n_classes, activation='softmax'))\n",
    "            loss = 'categorical_crossentropy'\n",
    "            metrics = ['accuracy']\n",
    "        opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "        model.compile(optimizer=opt, loss=loss, metrics=metrics)\n",
    "        return model\n",
    "\n",
    "    model = build_dnn(input_dim=input_dim, n_classes=n_classes)\n",
    "    model.summary()\n",
    "\n",
    "    # Prepare y for Keras\n",
    "    if n_classes == 2:\n",
    "        y_train_keras = np.array(y_train)\n",
    "        y_val_keras = np.array(y_val)\n",
    "    else:\n",
    "        from tensorflow.keras.utils import to_categorical\n",
    "        y_train_keras = to_categorical(y_train, num_classes=n_classes)\n",
    "        y_val_keras = to_categorical(y_val, num_classes=n_classes)\n",
    "\n",
    "    # Callbacks\n",
    "    es = callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1)\n",
    "    mc = callbacks.ModelCheckpoint(BEST_MODEL_PATH, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "\n",
    "    # Train with or without class weights\n",
    "    fit_kwargs = dict(\n",
    "        x=X_train, y=y_train_keras,\n",
    "        validation_data=(X_val, y_val_keras),\n",
    "        epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[es, mc], verbose=2\n",
    "    )\n",
    "    if class_weight_dict is not None:\n",
    "        fit_kwargs[\"class_weight\"] = class_weight_dict\n",
    "\n",
    "    history = model.fit(**fit_kwargs)\n",
    "\n",
    "    # Save final model\n",
    "    model.save(MODEL_PATH)\n",
    "    print(f\"Saved final model to {MODEL_PATH}\")\n",
    "    print(f\"Best model (checkpoint) saved to {BEST_MODEL_PATH}\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    if n_classes == 2:\n",
    "        y_test_prob = model.predict(X_test).ravel()\n",
    "        y_test_pred = (y_test_prob >= 0.5).astype(int)\n",
    "    else:\n",
    "        y_test_prob = model.predict(X_test)\n",
    "        y_test_pred = np.argmax(y_test_prob, axis=1)\n",
    "\n",
    "else:\n",
    "    # sklearn MLP fallback\n",
    "    print(\"Keras not available — using sklearn MLPClassifier (may be slower for many classes/features).\")\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(256,128,64), activation='relu', solver='adam',\n",
    "                        batch_size=min(128, X_train.shape[0]), learning_rate_init=LEARNING_RATE,\n",
    "                        max_iter=200, early_stopping=True, n_iter_no_change=10, random_state=RANDOM_STATE, verbose=True)\n",
    "\n",
    "    if class_weight_dict is not None:\n",
    "        # sklearn MLP doesn't accept class_weight directly for MLPClassifier — you'd need other models (e.g., balanced subsampling)\n",
    "        print(\"Note: sklearn MLPClassifier does not accept class_weight natively. Consider resampling or using a different classifier.\")\n",
    "    mlp.fit(X_train, y_train)\n",
    "    # Save model with joblib\n",
    "    try:\n",
    "        import joblib\n",
    "        joblib.dump(mlp, os.path.join(MODEL_DIR, \"mlp_sklearn.joblib\"))\n",
    "    except Exception as e:\n",
    "        print(\"Could not save sklearn model via joblib:\", e)\n",
    "    y_test_pred = mlp.predict(X_test)\n",
    "\n",
    "# =========================================================\n",
    "# 8. Evaluation: accuracy, classification report, confusion matrix, macro F1\n",
    "# =========================================================\n",
    "print(\"\\n=== Evaluation on test set ===\")\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "macro_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "print(f\"Test accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test macro F1: {macro_f1:.4f}\")\n",
    "\n",
    "print(\"\\nClassification report (per-class precision/recall/f1):\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=[str(c) for c in class_names], zero_division=0))\n",
    "\n",
    "# Confusion matrix (may be large)\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"Confusion matrix shape:\", cm.shape)\n",
    "\n",
    "# Visualize confusion matrix for small number of classes\n",
    "if n_classes <= 30:\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.ylabel('True')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping confusion matrix heatmap (too many classes).\")\n",
    "\n",
    "# =========================================================\n",
    "# 9. Plot training curves (if Keras used)\n",
    "# =========================================================\n",
    "if USE_KERAS and 'history' in globals():\n",
    "    h = history.history\n",
    "    # Accuracy curve\n",
    "    plt.figure()\n",
    "    if 'accuracy' in h:\n",
    "        plt.plot(h['accuracy'], label='train_accuracy')\n",
    "        plt.plot(h.get('val_accuracy', []), label='val_accuracy')\n",
    "        plt.title(\"Accuracy\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    # Loss curve\n",
    "    plt.figure()\n",
    "    plt.plot(h['loss'], label='train_loss')\n",
    "    plt.plot(h.get('val_loss', []), label='val_loss')\n",
    "    plt.title(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# =========================================================\n",
    "# 10. Save preprocessing artifacts for later inference\n",
    "# =========================================================\n",
    "artifacts = {\n",
    "    \"scaler\": scaler,\n",
    "    \"label_encoder\": label_encoder,\n",
    "    \"pca\": pca,\n",
    "    \"input_dim\": input_dim,\n",
    "    \"class_names\": list(class_names),\n",
    "    \"trained_on\": str(datetime.utcnow())\n",
    "}\n",
    "with open(ARTIFACTS_PATH, \"wb\") as f:\n",
    "    pickle.dump(artifacts, f)\n",
    "print(f\"Saved preprocessing artifacts to: {ARTIFACTS_PATH}\")\n",
    "\n",
    "# For convenience, also dump a small JSON summary\n",
    "summary = {\n",
    "    \"n_samples\": int(df.shape[0]),\n",
    "    \"n_features_original\": int(X.shape[1]),\n",
    "    \"n_features_final\": int(input_dim),\n",
    "    \"n_classes\": int(n_classes),\n",
    "    \"model_path\": MODEL_PATH if USE_KERAS else os.path.join(MODEL_DIR, \"mlp_sklearn.joblib\"),\n",
    "    \"artifacts_path\": ARTIFACTS_PATH\n",
    "}\n",
    "with open(os.path.join(MODEL_DIR, \"run_summary.json\"), \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(\"Wrote run summary to model_artifacts/run_summary.json\")\n",
    "\n",
    "# =========================================================\n",
    "# 11. Quick inference example (how to use saved artifacts & model)\n",
    "# =========================================================\n",
    "print(\"\\n=== Quick inference example (code snippet) ===\")\n",
    "print(\"\"\"\n",
    "# Example: Load artifacts and make predictions on new_data (pandas DataFrame with same columns as original X)\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow import keras  # if using Keras\n",
    "\n",
    "artifacts = pickle.load(open(\"model_artifacts/preprocessing_artifacts.pkl\", \"rb\"))\n",
    "scaler = artifacts['scaler']\n",
    "label_encoder = artifacts['label_encoder']\n",
    "pca = artifacts.get('pca', None)\n",
    "\n",
    "# new_df: a pandas DataFrame row(s)\n",
    "X_new = new_df.copy()\n",
    "# apply same preprocessing: label encoding for categorical columns (you must reproduce same mapping), scaling, PCA (if used)\n",
    "X_new_scaled = scaler.transform(X_new.values)\n",
    "if pca is not None:\n",
    "    X_new_reduced = pca.transform(X_new_scaled)\n",
    "else:\n",
    "    X_new_reduced = X_new_scaled\n",
    "\n",
    "# load model and predict\n",
    "model = keras.models.load_model(\"model_artifacts/dnn_model_best.h5\")\n",
    "y_prob = model.predict(X_new_reduced)\n",
    "if y_prob.shape[1] == 1:  # binary\n",
    "    preds = (y_prob.ravel() >= 0.5).astype(int)\n",
    "else:\n",
    "    preds = y_prob.argmax(axis=1)\n",
    "pred_labels = label_encoder.inverse_transform(preds)\n",
    "print(pred_labels)\n",
    "\"\"\")\n",
    "\n",
    "# =========================================================\n",
    "# 12. Final notes and suggestions\n",
    "# =========================================================\n",
    "print(\"\\n=== Final notes & suggestions ===\")\n",
    "print(\"\"\"\n",
    "- If you have many features (thousands), PCA or feature selection can speed training and reduce overfitting.\n",
    "- For heavy tabular tasks, tree-based models (RandomForest, XGBoost, LightGBM, CatBoost) often outperform simple DNNs.\n",
    "- If classes are imbalanced, options are:\n",
    "   * Use class weights (we computed them above for Keras),\n",
    "   * Resample: oversample minority (SMOTE) or undersample majority,\n",
    "   * Use focal loss (for keras) for extreme imbalance.\n",
    "- For hyperparameter tuning: try RandomizedSearchCV / KerasTuner to tune number of layers, units, dropout, lr, batch_size.\n",
    "- If GPU is available, Keras will use it (ensure proper TF / CUDA installation).\n",
    "- If training is slow / OOM: reduce batch size, reduce model size, or reduce PCA components.\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nPipeline complete. Check the 'model_artifacts' folder for saved model and preprocessing files.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
