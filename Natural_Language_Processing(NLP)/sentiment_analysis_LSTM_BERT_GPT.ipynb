{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c885b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:86: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AdamW' from 'transformers' (/usr/local/lib/python3.12/dist-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3262476723.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGPT2ForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'AdamW' from 'transformers' (/usr/local/lib/python3.12/dist-packages/transformers/__init__.py)",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Step 0: Install & Import Libraries\n",
    "# ===============================\n",
    "# pip install transformers torch tensorflow scikit-learn pandas\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77f92e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================\n",
    "# Step 2: LSTM Model\n",
    "# ===============================\n",
    "print(\"Training LSTM...\")\n",
    "\n",
    "# Tokenization & Padding\n",
    "max_len = 6\n",
    "lstm_tokenizer = Tokenizer()\n",
    "lstm_tokenizer.fit_on_texts(df[\"sentence\"])\n",
    "X_train_seq = pad_sequences(lstm_tokenizer.texts_to_sequences(train_texts), maxlen=max_len, padding=\"post\")\n",
    "X_test_seq = pad_sequences(lstm_tokenizer.texts_to_sequences(test_texts), maxlen=max_len, padding=\"post\")\n",
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)\n",
    "\n",
    "# Build LSTM model\n",
    "vocab_size = len(lstm_tokenizer.word_index) + 1\n",
    "lstm_model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=16, input_length=max_len),\n",
    "    LSTM(32),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "lstm_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "lstm_model.fit(X_train_seq, y_train, epochs=5, batch_size=8, verbose=0)\n",
    "\n",
    "# Evaluate LSTM\n",
    "lstm_preds = (lstm_model.predict(X_test_seq) > 0.5).astype(int)\n",
    "lstm_acc = accuracy_score(y_test, lstm_preds)\n",
    "print(f\"LSTM Accuracy: {lstm_acc:.4f}\")\n",
    "\n",
    "# ===============================\n",
    "# Step 3: BERT Model\n",
    "# ===============================\n",
    "print(\"Training BERT...\")\n",
    "\n",
    "# Tokenizer & Encodings\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train_encodings = bert_tokenizer(list(train_texts), truncation=True, padding=True, max_length=32, return_tensors=\"pt\")\n",
    "test_encodings = bert_tokenizer(list(test_texts), truncation=True, padding=True, max_length=32, return_tensors=\"pt\")\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "train_dataset = SentimentDataset(train_encodings, list(train_labels))\n",
    "test_dataset = SentimentDataset(test_encodings, list(test_labels))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "bert_model.to(device)\n",
    "optimizer = AdamW(bert_model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "bert_model.train()\n",
    "for epoch in range(3):\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        outputs = bert_model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate BERT\n",
    "bert_model.eval()\n",
    "preds, true_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        preds.extend(predictions.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "bert_acc = accuracy_score(true_labels, preds)\n",
    "print(f\"BERT Accuracy: {bert_acc:.4f}\")\n",
    "\n",
    "# ===============================\n",
    "# Step 4: GPT Model\n",
    "# ===============================\n",
    "print(\"Training GPT-2...\")\n",
    "\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
    "train_encodings = gpt_tokenizer(list(train_texts), truncation=True, padding=True, max_length=32, return_tensors=\"pt\")\n",
    "test_encodings = gpt_tokenizer(list(test_texts), truncation=True, padding=True, max_length=32, return_tensors=\"pt\")\n",
    "\n",
    "train_dataset = SentimentDataset(train_encodings, list(train_labels))\n",
    "test_dataset = SentimentDataset(test_encodings, list(test_labels))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "gpt_model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\n",
    "gpt_model.config.pad_token_id = gpt_model.config.eos_token_id\n",
    "gpt_model.to(device)\n",
    "optimizer = AdamW(gpt_model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "gpt_model.train()\n",
    "for epoch in range(3):\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        outputs = gpt_model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate GPT\n",
    "gpt_model.eval()\n",
    "preds, true_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        outputs = gpt_model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        preds.extend(predictions.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "gpt_acc = accuracy_score(true_labels, preds)\n",
    "print(f\"GPT-2 Accuracy: {gpt_acc:.4f}\")\n",
    "\n",
    "# ===============================\n",
    "# Step 5: Summary\n",
    "# ===============================\n",
    "print(\"\\nâœ… Accuracy Comparison:\")\n",
    "print(f\"LSTM: {lstm_acc:.4f}\")\n",
    "print(f\"BERT: {bert_acc:.4f}\")\n",
    "print(f\"GPT-2: {gpt_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
