{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c885b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Step 0: Install & Import Libraries\n",
    "# ===============================\n",
    "# pip install transformers torch tensorflow scikit-learn pandas\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49a0c172",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================\n",
    "# Step 1: Generate Dataset\n",
    "# ===============================\n",
    "sentences = [\n",
    "    \"I love this movie\", \"This film was amazing\", \"I enjoyed every moment\",\n",
    "    \"The acting was great\", \"What a fantastic experience\",\n",
    "    \"Absolutely wonderful movie\", \"The story was touching\", \"I liked the characters\",\n",
    "    \"Very entertaining film\", \"This movie made me happy\",\n",
    "    \n",
    "    \"I hate this movie\", \"This film was terrible\", \"I disliked every moment\",\n",
    "    \"The acting was awful\", \"What a boring experience\",\n",
    "    \"Absolutely horrible movie\", \"The story was weak\", \"I hated the characters\",\n",
    "    \"Very disappointing film\", \"This movie made me angry\"\n",
    "] * 5  # 100 samples\n",
    "\n",
    "labels = [\"positive\"] * 50 + [\"negative\"] * 50\n",
    "df = pd.DataFrame({\"sentence\": sentences, \"label\": labels})\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"label_encoded\"] = label_encoder.fit_transform(df[\"label\"])  # positive=1, negative=0\n",
    "\n",
    "# Split dataset\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df[\"sentence\"], df[\"label_encoded\"], test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1405531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 871ms/step\n",
      "LSTM Accuracy: 0.4000\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Step 2: LSTM Model\n",
    "# ===============================\n",
    "print(\"Training LSTM...\")\n",
    "\n",
    "# Tokenization & Padding\n",
    "max_len = 6\n",
    "lstm_tokenizer = Tokenizer()\n",
    "lstm_tokenizer.fit_on_texts(df[\"sentence\"])\n",
    "X_train_seq = pad_sequences(lstm_tokenizer.texts_to_sequences(train_texts), maxlen=max_len, padding=\"post\")\n",
    "X_test_seq = pad_sequences(lstm_tokenizer.texts_to_sequences(test_texts), maxlen=max_len, padding=\"post\")\n",
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)\n",
    "\n",
    "# Build LSTM model\n",
    "vocab_size = len(lstm_tokenizer.word_index) + 1\n",
    "lstm_model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=16, input_length=max_len),\n",
    "    LSTM(32),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "lstm_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "lstm_model.fit(X_train_seq, y_train, epochs=5, batch_size=8, verbose=0)\n",
    "\n",
    "# Evaluate LSTM\n",
    "lstm_preds = (lstm_model.predict(X_test_seq) > 0.5).astype(int)\n",
    "lstm_acc = accuracy_score(y_test, lstm_preds)\n",
    "print(f\"LSTM Accuracy: {lstm_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d36dee91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Accuracy: 0.6000\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Step 3: BERT Model\n",
    "# ===============================\n",
    "print(\"Training BERT...\")\n",
    "\n",
    "# Tokenizer & Encodings\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train_encodings = bert_tokenizer(list(train_texts), truncation=True, padding=True, max_length=32, return_tensors=\"pt\")\n",
    "test_encodings = bert_tokenizer(list(test_texts), truncation=True, padding=True, max_length=32, return_tensors=\"pt\")\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "train_dataset = SentimentDataset(train_encodings, list(train_labels))\n",
    "test_dataset = SentimentDataset(test_encodings, list(test_labels))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "bert_model.to(device)\n",
    "optimizer = AdamW(bert_model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "bert_model.train()\n",
    "for epoch in range(3):\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        outputs = bert_model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate BERT\n",
    "bert_model.eval()\n",
    "preds, true_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        preds.extend(predictions.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "bert_acc = accuracy_score(true_labels, preds)\n",
    "print(f\"BERT Accuracy: {bert_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "119bc8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GPT-2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c81493f2a81c4977af9cfce8effa10cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55cb609386da49379abdf43dd40f8dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a09fde64854d5295ce91147a0a3dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a903d26cbda743d983237d6e5813e240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82a872bb21344d609646554a7815ee7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbfbae82157c461193ffb066fc5ef053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 Accuracy: 0.4500\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Step 4: GPT Model\n",
    "# ===============================\n",
    "print(\"Training GPT-2...\")\n",
    "\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
    "train_encodings = gpt_tokenizer(list(train_texts), truncation=True, padding=True, max_length=32, return_tensors=\"pt\")\n",
    "test_encodings = gpt_tokenizer(list(test_texts), truncation=True, padding=True, max_length=32, return_tensors=\"pt\")\n",
    "\n",
    "train_dataset = SentimentDataset(train_encodings, list(train_labels))\n",
    "test_dataset = SentimentDataset(test_encodings, list(test_labels))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "gpt_model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\n",
    "gpt_model.config.pad_token_id = gpt_model.config.eos_token_id\n",
    "gpt_model.to(device)\n",
    "optimizer = AdamW(gpt_model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "gpt_model.train()\n",
    "for epoch in range(3):\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        outputs = gpt_model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate GPT\n",
    "gpt_model.eval()\n",
    "preds, true_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        outputs = gpt_model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        preds.extend(predictions.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "gpt_acc = accuracy_score(true_labels, preds)\n",
    "print(f\"GPT-2 Accuracy: {gpt_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd27029a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Accuracy Comparison:\n",
      "LSTM: 0.4000\n",
      "BERT: 0.6000\n",
      "GPT-2: 0.4500\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Step 5: Summary\n",
    "# ===============================\n",
    "print(\"\\n✅ Accuracy Comparison:\")\n",
    "print(f\"LSTM: {lstm_acc:.4f}\")\n",
    "print(f\"BERT: {bert_acc:.4f}\")\n",
    "print(f\"GPT-2: {gpt_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
