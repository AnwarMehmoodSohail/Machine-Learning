{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3a89478",
   "metadata": {},
   "source": [
    "\n",
    "Step 0: Import Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be803e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ce7eda",
   "metadata": {},
   "source": [
    "Step 1: Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "115fee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 positive and 50 negative sentences (total = 100)\n",
    "sentences = [\n",
    "    \"I love this movie\", \"This film was amazing\", \"I enjoyed every moment\",\n",
    "    \"The acting was great\", \"What a fantastic experience\",\n",
    "    \"Absolutely wonderful movie\", \"The story was touching\", \"I liked the characters\",\n",
    "    \"Very entertaining film\", \"This movie made me happy\",\n",
    "\n",
    "    \"I hate this movie\", \"This film was terrible\", \"I disliked every moment\",\n",
    "    \"The acting was awful\", \"What a boring experience\",\n",
    "    \"Absolutely horrible movie\", \"The story was weak\", \"I hated the characters\",\n",
    "    \"Very disappointing film\", \"This movie made me angry\"\n",
    "] * 5\n",
    "\n",
    "labels = [\"positive\"] * 50 + [\"negative\"] * 50\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"sentence\": sentences,\n",
    "    \"sentiment\": labels\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8186bff1",
   "metadata": {},
   "source": [
    "Step 2: Encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e97a19e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# positive → 1, negative → 0\n",
    "df[\"label\"] = label_encoder.fit_transform(df[\"sentiment\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8281b5e5",
   "metadata": {},
   "source": [
    "Step 3: Train–Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "117452e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"sentence\"].values,\n",
    "    df[\"label\"].values,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8680a71a",
   "metadata": {},
   "source": [
    "Step 4: Load BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e5e0665",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e96ad4c6f81b4ecea7dbdb52ee59eb26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d41bafcbc64bbb96a378b27be26ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60fe11f751a344b9a44aeb427c41e806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa34f2f045e4814b0b3e2ba1c2be5fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ae606d",
   "metadata": {},
   "source": [
    "Step 5: Tokenization + Padding (BERT Style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61ecdd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_tokenize(texts, labels, max_len=64):\n",
    "    \"\"\"\n",
    "    Converts text data into BERT input format\n",
    "    \"\"\"\n",
    "    encodings = tokenizer(\n",
    "        list(texts),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return encodings, torch.tensor(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aac0bee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings, train_labels = bert_tokenize(X_train, y_train)\n",
    "test_encodings, test_labels = bert_tokenize(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68149b58",
   "metadata": {},
   "source": [
    "Step 6: Create PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68ffecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns one training example\n",
    "        \"\"\"\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c307149",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SentimentDataset(train_encodings, train_labels)\n",
    "test_dataset = SentimentDataset(test_encodings, test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43969d05",
   "metadata": {},
   "source": [
    "Step 7: Load BERT Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c68509e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5985f600414b57b4e0805de92afc4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4882452e",
   "metadata": {},
   "source": [
    "Step 8: Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8081832",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690e8d08",
   "metadata": {},
   "source": [
    "Step 9: Train BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec623c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Training Loss: 7.0570\n",
      "Epoch 2 | Training Loss: 6.8590\n",
      "Epoch 3 | Training Loss: 6.9153\n"
     ]
    }
   ],
   "source": [
    "epochs = 3  # BERT needs fewer epochs\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Training Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786b2c51",
   "metadata": {},
   "source": [
    "Step 10: Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cd01f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "        predictions.extend(preds)\n",
    "        true_labels.extend(batch[\"labels\"].numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "233318aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.25\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102779a0",
   "metadata": {},
   "source": [
    "Step 11: Predict on New Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "407bca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment_bert(text):\n",
    "    \"\"\"\n",
    "    Predict sentiment using trained BERT model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=64\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "    return \"Positive\" if prediction == 1 else \"Negative\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c544bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "print(predict_sentiment_bert(\"I really enjoyed this movie\"))\n",
    "print(predict_sentiment_bert(\"This film was very boring\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
